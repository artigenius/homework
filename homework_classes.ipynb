{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artigenius/homeworke/blob/main/homework_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание. Задача 1\n",
        "\n",
        "Создать класс NERStats, который инициализирует документ и NERExtractor. Метод analyze_entities извлекает именованные сущности и создает для них частотный словарь. Метод display_most_common_entities отображает n самых частотных именованных сущностей.\n",
        "Извлекаем именованные сущности с использованием NERExtractor\n",
        "Создаем частотный словарь для именованных сущностей\n",
        "Отображаем n самых частотных именованных сущностей\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "*Небольшой комментарий:*\n",
        "Расчет сущностей происходит очень странно, но, в ходе исследования стало понятно, что так происходит из-за промпта"
      ],
      "metadata": {
        "id": "i_kjww3DpkPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "class NERExtractor:\n",
        "    def __init__(self):\n",
        "      self.nlp = spacy.load('en_core_web_sm')\n",
        "      self.named_entities = [] # создаем пустой список, который будем заполнять после\n",
        "\n",
        "    def extract_named_entities(self, text):\n",
        "      doc = self.nlp(text) # токенизация текста с помощью спайсу\n",
        "      entities = [(ent.text, ent.label_) for ent in doc.ents] # генератор списка именнованный сущностей\n",
        "      return entities\n",
        "\n",
        "    def display_named_entities(self):\n",
        "      for text, label in self.named_entities: # вызываем список из класса для взаимодействия с его элементами\n",
        "        print(f'Named entities: {text} - {label}')"
      ],
      "metadata": {
        "id": "CLXkhYnUFgaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERStats:\n",
        "  class_type = 'named entity extractor'\n",
        "  def __init__(self, text):\n",
        "    self.text = text\n",
        "    self.nerextractor = NERExtractor()\n",
        "\n",
        "  def analyze_entities(self, text):\n",
        "    \"\"\"\n",
        "    - извлекает сущности\n",
        "    - создает частотный словарь\n",
        "    \"\"\"\n",
        "    entities = self.nerextractor.extract_named_entities(self.text)\n",
        "    self.entity_count = Counter(entities)\n",
        "    return self.entity_count\n",
        "\n",
        "  def display_most_common_entities(self, n):\n",
        "    \"\"\"\n",
        "    - отображает н самых частотных сущностей\n",
        "    \"\"\"\n",
        "    most_common = self.entity_count.most_common(n)\n",
        "    for entity, count in most_common:\n",
        "      print(f'{entity}: {count}')\n",
        "\n",
        "\n",
        "text = 'SpaceX in 2023 is an aerospace manufacturer and space transport services company headquartered in California. It was founded in 2002, now it is 2023 by entrepreneur and investor Elon Musk, who has nothing to do with another famous creator Elon Musk who was born in California too.'\n",
        "ners = NERStats(text)\n",
        "ners.analyze_entities(text)\n",
        "ners.display_most_common_entities(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIzwHn_npkmP",
        "outputId": "e22b1fc3-8c94-40fb-a0b9-cf2993584a30"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('2023', 'DATE'): 2\n",
            "('California', 'GPE'): 2\n",
            "('Elon Musk', 'PERSON'): 2\n",
            "('2002', 'DATE'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание. Задача 2\n",
        "\n",
        "Создать класс LemmaStats, который инициализирует документ и Spacy. Метод lemmatize лемматизирует текст с помощью Spacy и создает для них частотный словарь. Метод display_most_common_lemmas отображает n самых частотных лемм.\n",
        "Лемматизируем текст с использованием Spacy\n",
        "Создаем частотный словарь для лемм\n",
        "Отображаем n самых частотных лемм\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "*Опять комментарий*\n",
        "Мне захотелось добавить стоп-слова, чтобы \"и\" или \"в\" не выходили в списке самых частотных лемм :)\n"
      ],
      "metadata": {
        "id": "iJLhbp9ypruH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords') # загрузим стоп-слова"
      ],
      "metadata": {
        "id": "pBJEEfmYQWfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LemmaStats:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "        self.nlp = spacy.load(\"ru_core_news_sm\")  # загружаем модель\n",
        "\n",
        "    def lemmatize(self):\n",
        "        doc = self.nlp(self.text)  # лемматизируем текст\n",
        "        self.lemmas = [token.lemma_ for token in doc if not token.is_punct]  # очищаем от пунктуации\n",
        "        stop_words = stopwords.words('russian')\n",
        "        self.lemmas_cleaned = [i for i in self.lemmas if i.lower() not in stop_words]  # убираем все стоп-слова чтобы выход был красивенький\n",
        "        self.lemma_counts = Counter(self.lemmas_cleaned)\n",
        "\n",
        "    def display_most_common_lemmas(self, n):\n",
        "        most_common = self.lemma_counts.most_common(n)  # получаем н самых частотных слов\n",
        "        print('Список самых частотных лемм:')\n",
        "        for lemma, count in most_common:\n",
        "            print(f'{lemma}: {count}')\n",
        "\n",
        "\n",
        "text = \"Дама сдавала в багаж Диван, Чемодан, Саквояж, Картину, Корзину, Картонку И маленькую собачонку. Выдали даме на станции Четыре зеленых квитанции О том, что получен багаж: Диван, Чемодан, Саквояж, Картина, Корзина, Картонка И маленькая собачонка.\"\n",
        "lemma_stats = LemmaStats(text)\n",
        "lemma_stats.lemmatize()\n",
        "lemma_stats.display_most_common_lemmas(8)\n"
      ],
      "metadata": {
        "id": "Fj1bC0jSpx3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173ae06c-0919-4e54-9d47-dd98bec20977"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Список самых частотных лемм:\n",
            "дама: 2\n",
            "багаж: 2\n",
            "диван: 2\n",
            "чемодан: 2\n",
            "саквояж: 2\n",
            "картина: 2\n",
            "корзина: 2\n",
            "маленький: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание. Задача 3\n",
        "\n",
        "Создать класс PoSStats, который инициализирует документ. Функция tokenize_and_tag токенизирует и размечает части речи с помощью NLTK (методы word_tokenize и pos_tag). Метод display_most_common_pos размечает тексты с помощью tokenize_and_tag и отображает n самых частотных частей речи.\n",
        "Токенизируем и размечаем части речи с использованием NLTK\n",
        "Отображаем n самых частотных частей речи\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "\n",
        "*Последний комментарий:*\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "мне пришлось загрузить, так как этого попросил коллаб, видимо, без них он не хотел работать"
      ],
      "metadata": {
        "id": "w-UM0pltpyXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "T3JspGJip4ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoSStats:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    def tokenize_and_tag(self):\n",
        "        tokens = word_tokenize(self.text)  # токенизируем текст\n",
        "        tagged = pos_tag(tokens)  # размечаем части речи\n",
        "        self.pos_counts = Counter(tag for word, tag in tagged)  # создаем частотный словарь для частей речи\n",
        "\n",
        "    def display_most_common_pos(self, n):\n",
        "        most_common = self.pos_counts.most_common(n)  # получаем н самых частотных частей речи\n",
        "        for pos, count in most_common:\n",
        "            print(f\"{pos}: {count}\")\n",
        "\n",
        "\n",
        "text = 'As a piano player, i would give anything to perform even one song on stage with these talented people'\n",
        "pos_stats = PoSStats(text)\n",
        "pos_stats.tokenize_and_tag()\n",
        "pos_stats.display_most_common_pos(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNIj51QuTp0O",
        "outputId": "23642153-fdda-4b0a-c060-e0562613f022"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN: 6\n",
            "IN: 3\n",
            "DT: 2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7uyQm17Z+liosBMCvbkJm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}