{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNDMOL+5FekhnuByALCdd6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artigenius/homeworke/blob/main/classwork/classwork_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMPfIqH_B-3H",
        "outputId": "450979db-42e1-4789-9f42-5bed0025182a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a tokenizer.\n"
          ]
        }
      ],
      "source": [
        "class WordTokenizer:\n",
        "  class_type = 'toke_tokenizer'\n",
        "  def __init__(self, text):\n",
        "    self.text = text\n",
        "\n",
        "  def tokenize(self): # examplar method\n",
        "    return self.text.lower().split()\n",
        "\n",
        "  # decorator - changes the features of the object,\n",
        "  # usual method --> class method\n",
        "  @classmethod\n",
        "  def sentence_tokenizer (cls, text):\n",
        "    return text.lower().split('.')\n",
        "\n",
        "  @staticmethod # some other method\n",
        "  def describe ():\n",
        "    print('This is a tokenizer.')\n",
        "\n",
        "\n",
        "tokenizer = WordTokenizer('my text is here be will')\n",
        "#print(tokenizer.tokenize())\n",
        "tokenizer.describe()\n",
        "#sentences = WordTokenizer.sentence_tokenizer('My text is here be will. And be than there one again')\n",
        "#print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Токенизатор текстов\n",
        "\"\"\"\n",
        "\n",
        "class WordTokenizer:\n",
        "    # задает тип объекта\n",
        "    class_type = \"tokenizer\"\n",
        "    def __init__(self, text):\n",
        "        # свойства объекта\n",
        "        self.text = text\n",
        "\n",
        "    def tokenize(self):\n",
        "        # метод для токенизации текста\n",
        "        return self.text.lower().split()\n",
        "\n",
        "    @classmethod\n",
        "    def sentence_tokenizer(cls, text):\n",
        "        # метод класса для токенизации предложений\n",
        "        return text.lower().split('. ')\n",
        "\n",
        "    @staticmethod\n",
        "    def describe():\n",
        "        # статический метод, описывает класс\n",
        "        print('This is a tokenizer.')\n",
        "\n",
        "# выведем тип объекта\n",
        "print(WordTokenizer.class_type)\n",
        "\n",
        "# создадим объект типа WordTokenizer\n",
        "tokenizer = WordTokenizer('This is a sample text')\n",
        "\n",
        "# токенизируем заданный текст\n",
        "print(tokenizer.tokenize())\n",
        "\n",
        "# токенизируем предложения, применим метод класса\n",
        "sentences = WordTokenizer.sentence_tokenizer('This is a sample text. This is another sentence')\n",
        "print(sentences)\n",
        "\n",
        "# опишем класс WordTokenizer с помощью статического метода\n",
        "tokenizer.describe()\n"
      ],
      "metadata": {
        "id": "-JRp5y2fMxaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}